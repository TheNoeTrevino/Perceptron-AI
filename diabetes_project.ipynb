{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries and Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\noetr\\AppData\\Local\\Temp\\ipykernel_22516\\804236364.py:3: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from imblearn.pipeline import Pipeline as ImPipeline\n",
    "from sklearn.impute import KNNImputer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   preg  plas  pres  skin  test  mass   pedi  age  class\n",
      "0     6   148    72    35     0  33.6  0.627   50      1\n",
      "1     1    85    66    29     0  26.6  0.351   31      0\n",
      "2     8   183    64     0     0  23.3  0.672   32      1\n",
      "3     1    89    66    23    94  28.1  0.167   21      0\n",
      "4     0   137    40    35   168  43.1  2.288   33      1\n"
     ]
    }
   ],
   "source": [
    "# assigning column names from dataset details\n",
    "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataset = pd.read_csv(url, names=names)\n",
    "X = dataset.iloc[:, :-1].values #all rows, EXCEPT last column for independent variable\n",
    "y = dataset.iloc[:, -1].values #all rows, and just last column for depent variable vector \n",
    "\n",
    "print(dataset.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all data is nominal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requirement 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the dataset into the Training, Test, and Validation sets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#60/20/20 training split. train/validation/test\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#splitting twice for 60/20/20 split\n",
    "X_leftover, X_test, y_leftover, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0) #800 leftover 200 test\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_leftover, y_leftover, test_size = 0.25, random_state = 0) #25% valid, 75% test, 200, 600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "460\n",
      "460\n",
      "154\n",
      "154\n",
      "154\n",
      "154\n"
     ]
    }
   ],
   "source": [
    "#checking lengths just in case\n",
    "print(len(X_train))\n",
    "print(len(y_train))\n",
    "print(len(X_valid))\n",
    "print(len(y_valid))\n",
    "print(len(X_test))\n",
    "print(len(y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taking Care of Misssing Data (R1.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking for Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plas      5\n",
      "pres     35\n",
      "skin    227\n",
      "test    374\n",
      "mass     11\n",
      "dtype: int64\n",
      "The dataset has 768 rows.\n"
     ]
    }
   ],
   "source": [
    "# checkl if there is any missing data\n",
    "columns_with_zeros_as_missing = ['plas', 'pres', 'skin', 'test', 'mass']\n",
    "\n",
    "missing_values = dataset[columns_with_zeros_as_missing].apply(lambda x: (x == 0).sum())\n",
    "print(missing_values)\n",
    "\n",
    "num_rows = dataset.shape[0]\n",
    "print(f\"The dataset has {num_rows} rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   preg  plas  pres  skin  test  mass   pedi  age  class\n",
      "0     6   148    72    35     0  33.6  0.627   50      1\n",
      "1     1    85    66    29     0  26.6  0.351   31      0\n",
      "2     8   183    64     0     0  23.3  0.672   32      1\n",
      "3     1    89    66    23    94  28.1  0.167   21      0\n",
      "4     0   137    40    35   168  43.1  2.288   33      1\n"
     ]
    }
   ],
   "source": [
    "print(dataset.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taking Care of Missing Data (a lot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### since there is so much missing data, i will use k nearest neighbors to fill in the missing data instead of just the mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turning object data types into numeric bc of errors \n",
    "for column in ['plas', 'pres', 'skin', 'test', 'mass']:\n",
    "    dataset[column] = pd.to_numeric(dataset[column], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preg       int64\n",
      "plas     float64\n",
      "pres     float64\n",
      "skin     float64\n",
      "test     float64\n",
      "mass     float64\n",
      "pedi     float64\n",
      "age        int64\n",
      "class      int64\n",
      "dtype: object\n",
      "   preg   plas  pres  skin   test  mass   pedi   age  class\n",
      "0   6.0  148.0  72.0  35.0  169.0  33.6  0.627  50.0    1.0\n",
      "1   1.0   85.0  66.0  29.0   58.6  26.6  0.351  31.0    0.0\n",
      "2   8.0  183.0  64.0  25.8  164.6  23.3  0.672  32.0    1.0\n",
      "3   1.0   89.0  66.0  23.0   94.0  28.1  0.167  21.0    0.0\n",
      "4   0.0  137.0  40.0  35.0  168.0  43.1  2.288  33.0    1.0\n"
     ]
    }
   ],
   "source": [
    "# large amounts of missing data awas found. We will use k nneighbors to best fill up the dataset\n",
    "columns_with_zeros_as_missing = ['plas', 'pres', 'skin', 'test', 'mass']\n",
    "dataset[columns_with_zeros_as_missing] = dataset[columns_with_zeros_as_missing].replace(0, np.nan)\n",
    "\n",
    "print(dataset.dtypes)\n",
    "\n",
    "imputer = KNNImputer(n_neighbors=5, weights='uniform')\n",
    "imputed_data = imputer.fit_transform(dataset)\n",
    "dataset_imputed = pd.DataFrame(imputed_data, columns=dataset.columns)\n",
    "\n",
    "# Display the head of the imputed DataFrame to verify changes\n",
    "print(dataset_imputed.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requirement 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Generating Perceptron Model and Deploying on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score: 0.6079399453244859\n",
      "f1 scores for each class: [0.67379679 0.49586777]\n"
     ]
    }
   ],
   "source": [
    "# create perceptron\n",
    "perceptron = Perceptron(max_iter=1000, tol=1e-3, random_state=0)\n",
    "\n",
    "perceptron.fit(X_train, y_train)\n",
    "y_pred_valid = perceptron.predict(X_valid)\n",
    "\n",
    "#printing out initial validation scores\n",
    "f1_scores_per_class = f1_score(y_valid, y_pred_valid, average=None)\n",
    "f1_score_weighted = f1_score(y_valid, y_pred_valid, average='weighted')\n",
    "print(\"f1 score:\", f1_score_weighted)\n",
    "print(\"f1 scores for each class:\", f1_scores_per_class)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requirement 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying SMOTE to Balance the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original dataset:  Counter({0: 296, 1: 164})\n",
      "resampled dataset:  Counter({0: 296, 1: 296})\n"
     ]
    }
   ],
   "source": [
    "smote = SMOTE(random_state=42)\n",
    "\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "print('original dataset: ', Counter(y_train))\n",
    "print('resampled dataset: ', Counter(y_train_resampled))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating NEW Linear Perceptron on the Oversampled Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making New Perceptron Model and Displaying Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "post smotenc f1 Score:\n",
      "post smotenc f1 Scores for Each Class: [0.67379679 0.49586777]\n"
     ]
    }
   ],
   "source": [
    "# generate new linear perceptron model on oversampled data set\n",
    "perceptron.fit(X_train_resampled, y_train_resampled)\n",
    "y_pred_test = perceptron.predict(X_test)\n",
    "\n",
    "#printing scores for each class post smotenc\n",
    "f1oversampled = f1_score(y_test, y_pred_test, average=None)\n",
    "f1_scores_per_class2 = f1_score(y_test, y_pred_test, average=None)\n",
    "print(f\"post smotenc f1 Score:\")\n",
    "print(\"post smotenc f1 Scores for Each Class:\", f1_scores_per_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requirement 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Idenfitying the Least Significant Predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pedi     0.173844\n",
      "pres     0.176665\n",
      "preg     0.221898\n",
      "age      0.238356\n",
      "skin     0.279530\n",
      "mass     0.313882\n",
      "test     0.320151\n",
      "plas     0.495853\n",
      "class    1.000000\n",
      "Name: class, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#using pandas lib to find correlations\n",
    "correlation_matrix = dataset_imputed.corr()\n",
    "target_correlation = correlation_matrix['class'].sort_values()\n",
    "print(target_correlation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping Two Least Significant Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping the features and displaying F values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the cell below this shit has to be fuckin wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   preg   plas  pres  skin   test  mass   pedi   age  class\n",
      "0   6.0  148.0  72.0  35.0  169.0  33.6  0.627  50.0    1.0\n",
      "1   1.0   85.0  66.0  29.0   58.6  26.6  0.351  31.0    0.0\n",
      "2   8.0  183.0  64.0  25.8  164.6  23.3  0.672  32.0    1.0\n",
      "3   1.0   89.0  66.0  23.0   94.0  28.1  0.167  21.0    0.0\n",
      "4   0.0  137.0  40.0  35.0  168.0  43.1  2.288  33.0    1.0\n"
     ]
    }
   ],
   "source": [
    "print(dataset_imputed.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 296, 1: 164})\n",
      "Counter({0: 296, 1: 296})\n"
     ]
    }
   ],
   "source": [
    "print(Counter(y_train))\n",
    "print(Counter(y_train_resampled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.7314\n",
      "F1 score for each class: [0.83555556 0.55421687]\n"
     ]
    }
   ],
   "source": [
    "#drop least 2 sig var: pedi pres\n",
    "dataset_reduced = dataset_imputed.drop(['pedi', 'pres'], axis=1)\n",
    "\n",
    "# redoing some preprocessing bc you dropped columns\n",
    "X = dataset_reduced.iloc[:, :-1].values\n",
    "y = dataset_reduced.iloc[:, -1].values   \n",
    "\n",
    "#splitting twice for 60/20/20 split\n",
    "X_leftover, X_test, y_leftover, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0) #800 leftover 200 test\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_leftover, y_leftover, test_size = 0.25, random_state = 0) #25% valid, 75% test, 200, 600\n",
    "\n",
    "# #redoing smote after the drop so the baby variables dont have it either. it should be the same otherwise tho\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# feature scaling\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_resampled)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# new perceptron again\n",
    "perceptron = Perceptron(max_iter=1000, tol=1e-3, random_state=0)\n",
    "perceptron.fit(X_train_scaled, y_train_resampled)\n",
    "\n",
    "# f1 scores\n",
    "y_pred_valid = perceptron.predict(X_valid_scaled)\n",
    "f1_valid = f1_score(y_valid, y_pred_valid, average='weighted')  # Using 'weighted' to handle class imbalance\n",
    "print(f\"F1 Score: {f1_valid:.4f}\")\n",
    "f1_valid = f1_score(y_valid, y_pred_valid, average=None)\n",
    "print(f\"F1 score for each class: {f1_valid}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "accuracy of class of interest was increased"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requirement 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify the Two Most Significant Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pres     0.170589\n",
      "pedi     0.173844\n",
      "preg     0.221898\n",
      "age      0.238356\n",
      "skin     0.259491\n",
      "test     0.303454\n",
      "mass     0.313680\n",
      "plas     0.494650\n",
      "class    1.000000\n",
      "Name: class, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "correlation_matrix = dataset.corr()\n",
    "target_correlation = correlation_matrix['class'].sort_values()\n",
    "print(target_correlation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requirement 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tunning the Oversampling Ratio and n-values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Grid Search on the Validation Set to find optimal tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0.0: 296, 1.0: 164})\n"
     ]
    }
   ],
   "source": [
    "print(Counter(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:  {'smote__k_neighbors': 6, 'smote__sampling_strategy': 1.0}\n",
      "F1 Score: 0.5252\n",
      "F1 Scores for Each Class : [0.48529412 0.59302326]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\noetr\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\model_selection\\_validation.py:542: FitFailedWarning: \n",
      "250 fits failed out of a total of 500.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "250 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\noetr\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\model_selection\\_validation.py\", line 890, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\noetr\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py\", line 1351, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\noetr\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\imblearn\\pipeline.py\", line 322, in fit\n",
      "    Xt, yt = self._fit(X, y, routed_params)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\noetr\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\imblearn\\pipeline.py\", line 258, in _fit\n",
      "    X, y, fitted_transformer = fit_resample_one_cached(\n",
      "                               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\noetr\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\joblib\\memory.py\", line 353, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\noetr\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\imblearn\\pipeline.py\", line 1050, in _fit_resample_one\n",
      "    X_res, y_res = sampler.fit_resample(X, y, **params.get(\"fit_resample\", {}))\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\noetr\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\imblearn\\base.py\", line 208, in fit_resample\n",
      "    return super().fit_resample(X, y)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\noetr\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\imblearn\\base.py\", line 108, in fit_resample\n",
      "    self.sampling_strategy_ = check_sampling_strategy(\n",
      "                              ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\noetr\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\imblearn\\utils\\_validation.py\", line 571, in check_sampling_strategy\n",
      "    _sampling_strategy_float(sampling_strategy, y, sampling_type).items()\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\noetr\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\imblearn\\utils\\_validation.py\", line 410, in _sampling_strategy_float\n",
      "    raise ValueError(\n",
      "ValueError: The specified ratio required to remove samples from the minority class while trying to generate new samples. Please increase the ratio.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\noetr\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\model_selection\\_search.py:1051: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan        nan 0.46472887\n",
      " 0.52744638 0.37754447 0.5145362  0.45976364        nan        nan\n",
      "        nan        nan        nan 0.49945145 0.45875575 0.46599277\n",
      " 0.38798639 0.44027427        nan        nan        nan        nan\n",
      "        nan 0.49532123 0.48617381 0.43880033 0.4384402  0.42838876\n",
      "        nan        nan        nan        nan        nan 0.42557431\n",
      " 0.55761944 0.49603577 0.42981263 0.36594522        nan        nan\n",
      "        nan        nan        nan 0.44858677 0.53912555 0.43787605\n",
      " 0.47847414 0.49479424        nan        nan        nan        nan\n",
      "        nan 0.3607475  0.53182833 0.42178526 0.43785354 0.56967019\n",
      "        nan        nan        nan        nan        nan 0.52789551\n",
      " 0.53843717 0.53233398 0.47322863 0.48684052        nan        nan\n",
      "        nan        nan        nan 0.42692603 0.49030307 0.51259453\n",
      " 0.47685782 0.41222149        nan        nan        nan        nan\n",
      "        nan 0.43502529 0.35805884 0.50984656 0.42824243 0.35005365\n",
      "        nan        nan        nan        nan        nan 0.4170891\n",
      " 0.50762216 0.36803605 0.53287694 0.48311263]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# setting up the pipline\n",
    "pipeline = ImPipeline([\n",
    "    ('smote', SMOTE(random_state=42)),\n",
    "    ('perceptron', Perceptron(max_iter=1000, tol=1e-3, random_state=0))\n",
    "])\n",
    "\n",
    "# setting neighbors for the search to try\n",
    "param_grid = {\n",
    "    'smote__sampling_strategy': [.1, .2, .3, .4, .5, .6, .7, .8, .9, 1.0],  # Adjust based on your dataset's imbalance\n",
    "    'smote__k_neighbors': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]  # Fewer values for initial testing\n",
    "}\n",
    "\n",
    "# actually doing the grid search\n",
    "grid_search = GridSearchCV(pipeline, param_grid, scoring='f1_weighted', cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n",
    "\n",
    "# using best estimator to predict on validation set\n",
    "best_estimator = grid_search.best_estimator_\n",
    "y_pred_valid = best_estimator.predict(X_valid)\n",
    "\n",
    "# printing f1 scores\n",
    "f1_valid = f1_score(y_valid, y_pred_valid, average='weighted')\n",
    "print(f\"F1 Score: {f1_valid:.4f}\")\n",
    "f1_scores_per_class_valid = f1_score(y_valid, y_pred_valid, average=None)\n",
    "print(\"F1 Scores for Each Class :\", f1_scores_per_class_valid)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
